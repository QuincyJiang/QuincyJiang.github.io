<!DOCTYPE html>
<html lang="en">
    <!-- title -->




<!-- keywords -->




<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" >
    <meta name="author" content="QuincyJiang">
    <meta name="renderer" content="webkit">
    <meta name="copyright" content="QuincyJiang">
    
    <meta name="keywords" content="binder,android,江夏秋">
    
    <meta name="description" content="">
    <meta http-equiv="Cache-control" content="no-cache">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
    <title>深度学习课程笔记 第一周 深度学习的实践层面 · 瘟疫青年</title>
    <style type="text/css">
    @font-face {
        font-family: 'Oswald-Regular';
        src: url("/font/Oswald-Regular.ttf");
    }

    body {
        margin: 0;
    }

    header,
    footer,
    .back-top,
    .sidebar,
    .container,
    .site-intro-meta,
    .toc-wrapper {
        display: none;
    }

    .site-intro {
        position: relative;
        z-index: 3;
        width: 100%;
        /* height: 50vh; */
        overflow: hidden;
    }

    .site-intro-placeholder {
        position: absolute;
        z-index: -2;
        top: 0;
        left: 0;
        width: calc(100% + 300px);
        height: 100%;
        background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
        background-position: center center;
        transform: translate3d(-226px, 0, 0);
        animation: gradient-move 2.5s ease-out 0s infinite;
    }

    @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }

</style>

    <link rel="preload" href= "/css/style.css?v=20180824" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    <link rel="stylesheet" href= "/css/mobile.css?v=20180824" media="(max-width: 980px)">
    
    <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'" />
    
    <!-- /*! loadCSS. [c]2017 Filament Group, Inc. MIT License */
/* This file is meant as a standalone workflow for
- testing support for link[rel=preload]
- enabling async CSS loading in browsers that do not support rel=preload
- applying rel preload css once loaded, whether supported or not.
*/ -->
<script>
(function( w ){
	"use strict";
	// rel=preload support test
	if( !w.loadCSS ){
		w.loadCSS = function(){};
	}
	// define on the loadCSS obj
	var rp = loadCSS.relpreload = {};
	// rel=preload feature support test
	// runs once and returns a function for compat purposes
	rp.support = (function(){
		var ret;
		try {
			ret = w.document.createElement( "link" ).relList.supports( "preload" );
		} catch (e) {
			ret = false;
		}
		return function(){
			return ret;
		};
	})();

	// if preload isn't supported, get an asynchronous load by using a non-matching media attribute
	// then change that media back to its intended value on load
	rp.bindMediaToggle = function( link ){
		// remember existing media attr for ultimate state, or default to 'all'
		var finalMedia = link.media || "all";

		function enableStylesheet(){
			link.media = finalMedia;
		}

		// bind load handlers to enable media
		if( link.addEventListener ){
			link.addEventListener( "load", enableStylesheet );
		} else if( link.attachEvent ){
			link.attachEvent( "onload", enableStylesheet );
		}

		// Set rel and non-applicable media type to start an async request
		// note: timeout allows this to happen async to let rendering continue in IE
		setTimeout(function(){
			link.rel = "stylesheet";
			link.media = "only x";
		});
		// also enable media after 3 seconds,
		// which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
		setTimeout( enableStylesheet, 3000 );
	};

	// loop through link elements in DOM
	rp.poly = function(){
		// double check this to prevent external calls from running
		if( rp.support() ){
			return;
		}
		var links = w.document.getElementsByTagName( "link" );
		for( var i = 0; i < links.length; i++ ){
			var link = links[ i ];
			// qualify links to those with rel=preload and as=style attrs
			if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
				// prevent rerunning on link
				link.setAttribute( "data-loadcss", true );
				// bind listeners to toggle media back
				rp.bindMediaToggle( link );
			}
		}
	};

	// if unsupported, run the polyfill
	if( !rp.support() ){
		// run once at least
		rp.poly();

		// rerun poly on an interval until onload
		var run = w.setInterval( rp.poly, 500 );
		if( w.addEventListener ){
			w.addEventListener( "load", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		} else if( w.attachEvent ){
			w.attachEvent( "onload", function(){
				rp.poly();
				w.clearInterval( run );
			} );
		}
	}


	// commonjs
	if( typeof exports !== "undefined" ){
		exports.loadCSS = loadCSS;
	}
	else {
		w.loadCSS = loadCSS;
	}
}( typeof global !== "undefined" ? global : this ) );
</script>

    <link rel="icon" href= "/images/logo.png" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js" as="script" />
    <link rel="preload" href="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" as="script" />
    <link rel="preload" href="/scripts/main.js" as="script" />
    <link rel="preload" as="font" href="/font/Oswald-Regular.ttf" crossorigin>
    <link rel="preload" as="font" href="https://at.alicdn.com/t/font_327081_1dta1rlogw17zaor.woff" crossorigin>
    
    <!-- fancybox -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  -->
    
</head>

    
        <body class="post-body">
    
    
<header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >瘟疫青年</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">深度学习课程笔记 第一周 深度学习的实践层面</a>
            </div>
    </div>
    
    <a class="home-link" href=/>瘟疫青年</a>
</header>
    <div class="wrapper">
        <div class="site-intro" style="







height:50vh;
">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-img" style="background-image: url(/intro/post-bg.jpg)"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            深度学习课程笔记 第一周 深度学习的实践层面
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <div class="post-intros">
                <!-- 文章页标签  -->
                
                    <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "机器学习">机器学习</a>
    
        <a class="post-tag" href="javascript:void(0);" data-tags = "Tensorflow">Tensorflow</a>
    
</div>
                
                
                    <div class="post-intro-read">
                        <span>Word count: <span class="post-count word-count">5.8k</span>Reading time: <span class="post-count reading-time">20 min</span></span>
                    </div>
                
                <div class="post-intro-meta">
                    <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                    <span class="post-intro-time">2018/11/15</span>
                    
                    <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                        <span class="iconfont-archer">&#xe602;</span>
                        <span id="busuanzi_value_page_pv"></span>
                    </span>
                    
                    <span class="shareWrapper">
                        <span class="iconfont-archer shareIcon">&#xe71d;</span>
                        <span class="shareText">Share</span>
                        <ul class="shareList">
                            <li class="iconfont-archer share-qr" data-type="qr">&#xe75b;
                                <div class="share-qrcode"></div>
                            </li>
                            <li class="iconfont-archer" data-type="weibo">&#xe619;</li>
                            <li class="iconfont-archer" data-type="qzone">&#xe62e;</li>
                            <li class="iconfont-archer" data-type="twitter">&#xe634;</li>
                            <li class="iconfont-archer" data-type="facebook">&#xe67a;</li>
                        </ul>
                    </span>
                </div>
            </div>
        
    </div>
</div>
        <script>
 
  // get user agent
  var browser = {
    versions: function () {
      var u = window.navigator.userAgent;
      return {
        userAgent: u,
        trident: u.indexOf('Trident') > -1, //IE内核
        presto: u.indexOf('Presto') > -1, //opera内核
        webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
        gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
        mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
        ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
        android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
        iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
        iPad: u.indexOf('iPad') > -1, //是否为iPad
        webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
        weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
        uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
      };
    }()
  }
  console.log("userAgent:" + browser.versions.userAgent);

  // callback
  function fontLoaded() {
    console.log('font loaded');
    if (document.getElementsByClassName('site-intro-meta')) {
      document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
      document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
      var postIntros = document.getElementsByClassName('post-intros')[0]
      if (postIntros) {
        postIntros.classList.add('post-fade-in');
      }
    }
  }

  // UC不支持跨域，所以直接显示
  function asyncCb(){
    if (browser.versions.uc) {
      console.log("UCBrowser");
      fontLoaded();
    } else {
      WebFont.load({
        custom: {
          families: ['Oswald-Regular']
        },
        loading: function () {  //所有字体开始加载
          // console.log('loading');
        },
        active: function () {  //所有字体已渲染
          fontLoaded();
        },
        inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
          console.log('inactive: timeout');
          fontLoaded();
        },
        timeout: 5000 // Set the timeout to two seconds
      });
    }
  }

  function asyncErr(){
    console.warn('script load from CDN failed, will load local script')
  }

  // load webfont-loader async, and add callback function
  function async(u, cb, err) {
    var d = document, t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (cb) { o.addEventListener('load', function (e) { cb(null, e); }, false); }
    if (err) { o.addEventListener('error', function (e) { err(null, e); }, false); }
    s.parentNode.insertBefore(o, s);
  }

  var asyncLoadWithFallBack = function(arr, success, reject) {
      var currReject = function(){
        reject()
        arr.shift()
        if(arr.length)
          async(arr[0], success, currReject)
        }

      async(arr[0], success, currReject)
  }

  asyncLoadWithFallBack([
    "https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.min.js", 
    "https://cdn.bootcss.com/webfont/1.6.28/webfontloader.js",
    "/lib/webfontloader.min.js"
  ], asyncCb, asyncErr)
</script>        
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <p>本博是吴恩达DeepLearning.ai 的学习笔记</p>
<h1 id="改善深层神经网络：-超参调试、正则化以及优化"><a href="#改善深层神经网络：-超参调试、正则化以及优化" class="headerlink" title="改善深层神经网络： 超参调试、正则化以及优化"></a>改善深层神经网络： 超参调试、正则化以及优化</h1><h2 id="训练集-开发集-测试集-的划分"><a href="#训练集-开发集-测试集-的划分" class="headerlink" title="训练集 开发集 测试集 的划分"></a>训练集 开发集 测试集 的划分</h2><p>假设所有的训练数据如下<br><img src="/media/15422471488916.jpg" alt=""><br>一个典型的数据划分：<br><img src="/media/15422472060092.jpg" alt=""></p>
<p>即：一部分作为 <strong>训练集</strong><br>一部分作为 <strong>简单交叉验证集/验证集</strong><br>一部分作为 <strong>测试集</strong></p>
<p>具体的流程是 在<strong>训练集</strong>上 对<strong>各模型运行训练算法</strong> 将训练<strong>完成的模型</strong> 带入 <strong>交叉验证集</strong> 选择出最佳模型<br>然后带入<strong>测试集</strong>对神经网络做出<strong>无偏评估</strong></p>
<p>小数据量的时候，一般会将数据划分为 <strong>训练集（70%）</strong> <strong>测试集（30%）</strong>或者<strong>训练集（60%）、验证集（20%、测试集（20%）</strong>。<br>在大数据量（百万级）时，验证集 测试集的数据量和小数据量的情况差不多，仍然可能需要成千上万，但是相比百万级的训练集来说，测试集所占的比例理所当然的变小了。比如我们拥有1000000，这么多的数据，<strong>训练集：验证集：测试集=98:1:1</strong>。</p>
<h2 id="保证-验证集合测试集具有相同的分布"><a href="#保证-验证集合测试集具有相同的分布" class="headerlink" title="保证 验证集合测试集具有相同的分布"></a>保证 验证集合测试集具有相同的分布</h2><p>有时候 训练集和测试集的数据来源往往不同，比如说 训练集可能是通过爬虫爬取的网页图片，但是测试集是来自用户通过app上传的图片，这些图片的质量和对齐方式参差不齐，这种情况 最好能保证 <strong>测试集和交叉验证集具有相同的分布。</strong><br><img src="/media/15422532146088.jpg" alt=""></p>
<h2 id="为什么要保证分布相同"><a href="#为什么要保证分布相同" class="headerlink" title="为什么要保证分布相同"></a>为什么要保证分布相同</h2><p>我们先从测试集和验证集的作用开始说起<br>当我们在拿到数据后，会把数据划分为三部分，将训练集丢给模型，这个步骤是为了进行梯度下降，以期得到模型参数。<br>然后拿到训练完成的模型，带入验证集，这个部分是为了检查模型是否能够很好的拟合验证数据，因为这部分数据是没有经过梯度下降的，可以说验证集和测试集没有交<br>集，测试的准确率是可靠的。</p>
<p>但是模型除了<strong>普通参数（w和b）</strong>之外，还有<strong>超参数</strong>的存在，当不引入强化学习的情况下，普通参数可以被梯度下降更新，也就是可以被训练集更新，但是为了提高模型性能，我们往往会对 <strong>神经网络层数、网络节点数、迭代次数、学习步长</strong>等进行调整，这些参数不受梯度下降的影响，一般都是根据验证集的表现情况进行人为调整。</p>
<p>所以可以说，验证集也对学习结果产生了影响，所以需要一份完全没有经过学习影响的数据，来评估最终模型的表现情况，这个就是测试集存在的意义。</p>
<p><strong>为什么要保证测试集和验证集有相同的分布呢？</strong></p>
<p>因为一旦定义好了测试集和验证集，开发人员的目的就是专注提高验证集的表现，这便要求验证集的选取可以分布均匀，可以体现核心任务。如果验证集和测试集分布不同，就可能导致 <strong>系统在验证集上表现良好，在测试集表现不好。</strong>这种情况可能会有多种原因：</p>
<ul>
<li>算法在开发集上过拟合了。</li>
<li>测试集比开发集更难进行预测，尽管算法做得足够好了，却很难有进一步的提升空间。</li>
<li>测试集不一定更难预测，但它与开发集性质并不相同（分布不同）。<br>因此在开发集上表现良好的算法不一定在测试集上也能够取得出色表现。<br>这样就引入了新的不确定性–<strong>提高算法在验证集的表现，是否能提高其在测试集的表现？</strong>如果是这种情况，大量针对开发集性能的改进工作将会是徒劳的。</li>
</ul>
<h2 id="如何评估模型表现？-偏差、方差"><a href="#如何评估模型表现？-偏差、方差" class="headerlink" title="如何评估模型表现？ 偏差、方差"></a>如何评估模型表现？ 偏差、方差</h2><p><strong>偏差(<em>bias</em>)：</strong>可以理解为 模型在训练集的表现不佳 也就是模型无法很好的拟合数据  称之为欠拟合<br><strong>方差(<em>variance</em>)：</strong>模型在训练集表现良好，但在测试集表现差，模型过于拟合了训练集数据导致无法正确反映数据规律，称之为 过拟合</p>
<p><img src="/media/15422574024898.jpg" alt=""></p>
<p>在实际中，判断偏差和方差一般会通过 训练集和测试集误差来判断</p>
<p><img src="/media/15422578079564.jpg" alt=""></p>
<p>从左到右:</p>
<ul>
<li><strong>高方差</strong>：模型过拟合了</li>
<li><strong>高偏差</strong>：模型没有被很好的训练 但是貌似没有过拟合 </li>
<li><strong>高方差 高偏差：</strong> 模型没有很好拟合数据 同时在测试集表现也不佳 有时有些高维数据中会出现这种情况，就是在某些区域的偏差高，有些区域的方差高</li>
<li><strong>低方差 低偏差：</strong> 模型可以很好表现数据特征</li>
</ul>
<p>即：<br><strong>训练集误差高 表示 偏差大<br>测试集误差高 表示 方差大</strong></p>
<p><strong>关于方差和偏差的数学原理 ，见文章附录[1]。</strong></p>
<h2 id="一个基本流程"><a href="#一个基本流程" class="headerlink" title="一个基本流程"></a>一个基本流程</h2><p><img src="/media/15422648475258.jpg" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: 获取数据</span><br><span class="line">e=&gt;end: 训练结束</span><br><span class="line">op=&gt;operation: 评估训练集误差（偏差值）</span><br><span class="line">op1=&gt;operation: 评估测试集误差（方差值）</span><br><span class="line">op2=&gt;operation: 选择更深层的神经网络或花费更多时间训练算法，或尝试更先进算法</span><br><span class="line">op3=&gt;operation: 获取更多数据，或者通过正则化减少过拟合</span><br><span class="line">cond=&gt;condition: 偏差正常?</span><br><span class="line">cond2=&gt;condition: 方差偏高?</span><br><span class="line"></span><br><span class="line">st-&gt;op</span><br><span class="line">op-&gt;cond</span><br><span class="line">cond(yes)-&gt;op1</span><br><span class="line">cond(no)-&gt;op2</span><br><span class="line">op2-&gt;cond</span><br><span class="line">op1-&gt;cond2</span><br><span class="line">cond2(yes)-&gt;op3</span><br><span class="line">cond2(no)-&gt;e</span><br><span class="line">op3-&gt;cond2</span><br></pre></td></tr></table></figure>
<p>在早期机器学习的时代，我们没有太多工具可以只影响偏差或者方差的一种而不对另一种造成影响，所以往往需要在两者间权衡。<br>现在在大数据时代，我们有了一些工具，比如只要对数据进行适当正则，再构建一个更大的神经网络，就可以在几乎不影响方差的情况下，减少偏差。而采用更多数据，或者对数据正则化，可以在不过多影响偏差的情况下，减少方差</p>
<h2 id="降低方差的手段：正则化"><a href="#降低方差的手段：正则化" class="headerlink" title="降低方差的手段：正则化"></a>降低方差的手段：正则化</h2><p>出现高方差往往就表示，模型对数据过拟合了。<br>前面讲到 降低方差的手段，可以通过获取更多数据或者对数据进行正则来实现。</p>
<p>更多数据的获取，实现起来成本比较高，那正则化又为什么可以减少方差呢？</p>
<h3 id="如何实施正则化"><a href="#如何实施正则化" class="headerlink" title="如何实施正则化"></a>如何实施正则化</h3><h4 id="logistic-回归"><a href="#logistic-回归" class="headerlink" title="logistic 回归"></a>logistic 回归</h4><p>我们之前定义的损失函数为<br><img src="/media/15422655692214.jpg" alt=""></p>
<p>现在在其基础上加上正则化参数 也就是向量w的二范数（也有使用一范数的，但目前更多倾向于使用二范数）</p>
<p><img src="/media/15422656218635.jpg" alt=""></p>
<p>至于b的正则化参数，可加可不加，因为w通常是一个高维度的矩阵，它已经包含很多参数了，b只是众多参数的其中之一，加不加影响也不是很大。<br><img src="/media/15422656880309.jpg" alt=""></p>
<p>其中 λ是正则化参数。这个参数需要我们在验证集经过多次调试来选择最佳取值，λ属于超参数的一种。</p>
<h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p>神经网络和logistic回归的差别在于，神经网络是有多层隐藏神经元的数学模型。</p>
<p>它的正则化参数则是将所有层的w向量写成一个矩阵后 矩阵W的二范数，<strong>在DL中，习惯将矩阵的二范数称之为 F范数，或者弗罗贝尼乌斯范数</strong><br><img src="/media/15422662423114.jpg" alt=""></p>
<p>在之前的梯度下降过程中，我们通过反向传播 计算出了原始J对w[l]的导数：<br><img src="/media/15422667161032.jpg" alt="-w234"><br>现在代价函数J增加了一项正则化参数<br>对正则化参数求导<br><img src="/media/15422666818228.jpg" alt="-w252"></p>
<p>为<img src="/media/15422667406636.jpg" alt="-w136"></p>
<p>则新的dw[l]为</p>
<p><img src="/media/15422668045079.jpg" alt="-w428"></p>
<p>带入<br><code>w[l] = w[l]-α*dw[l]</code><br>得到<br><img src="/media/15422668622209.jpg" alt="-w531"></p>
<p><img src="/media/15422669165717.jpg" alt="-w569"></p>
<p>可以看到，如果将w[l]提取出来，每次反向传播后，更新的权重值相当于在其前面乘上了这么一个参数：<br><img src="/media/15422670345410.jpg" alt="-w170"></p>
<p>显而易见这个参数是小于1的。</p>
<p>所以L2正则化也被称之为 <strong>权重衰减</strong></p>
<p>所以 对神经网络 应用L2 正则化的过程 其实相当于在每次反向传播更新梯度的过程中，对w[l]乘上了一个小于1的参数<br><code>1-αλ/m</code><br>这个参数是小于1的，所以 L2正则化 也被称之为 <strong>权重衰减</strong>。</p>
<h3 id="为什么正则化可以减少过拟合"><a href="#为什么正则化可以减少过拟合" class="headerlink" title="为什么正则化可以减少过拟合"></a>为什么正则化可以减少过拟合</h3><p>从直观上理解，因为我们的代价函数加上了对权重矩阵W的正则化参数，也就是W的F范数，在训练过程中，为了降低代价函数J，必然会压缩正则化参数，当λ设置的足够大，会导致矩阵W的F范数接近于0，也就意味着W中的很多元素变为了0，<br>等于原始神经网络中的很多神经元失去了作用，模型也被精简了。</p>
<p><img src="/media/15422676113907.jpg" alt="-w401"></p>
<p>此时模型越来越趋近于logistic回归<br>模型过于简单的时候，会导致偏差变高，模型甚至无法很好的拟合数据。<br>但这种W中参数变为0的情况在现实情况下一般不会发生，往往是某些w会变得很小，等于在训练过程中，这些神经元没有消失，只不过权重值变得很小而已。这样来看，貌似神经网络变得更简单了，这样更不容易出现过拟合。</p>
<p>下面举一个直观一些的例子。</p>
<p>假设每一层的激活函数都是tanh(),tanh()函数具有一个特殊性质，就是当x范围比较小的时候，tanh()接近y=x。<br><img src="/media/15422686564038.jpg" alt="-w675"></p>
<p>当我们对神经网络施加L2正则化，在训练中会导致w变小，又因为g(z)中的z 等于<br><img src="/media/15422687425629.jpg" alt="-w391"></p>
<p>当w[l]变小，会导致z[l]变小，当z落在tanh的中间那一小部分范围时，整个神经网络只利用到了tanh的线性部分，模型也会趋近于线性回归。这样会有效降低过拟合出现的情况</p>
<h2 id="另一种正则化方式：-Dropout正则化"><a href="#另一种正则化方式：-Dropout正则化" class="headerlink" title="另一种正则化方式： Dropout正则化"></a>另一种正则化方式： Dropout正则化</h2><p>Dropout，也被称之为随机失活。<br>其实就是 令神经网络的每一层的每个神经元，按照一定的概率失活。<br>我们将原始训练集分为若干子集，对每一个子集，都利用一次dropout<br><img src="/media/15422763982130.jpg" alt="-w451"></p>
<p>假设在这个神经网络中，我们设置每个元素的失活概率为0.5，那么在这个子集中，我们便会得到一个精简的神经网络<br><img src="/media/15422764818132.jpg" alt="-w449"></p>
<p>对每一个训练子集机型dropout，便会得到很多精简的神经网络，我们对每一个神经网络进行训练。其实这里也能看出来，为什么dropout可以有效防止过拟合，以为实施过dropout的神经网络变小了。</p>
<p>下面以python代码示例 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 假设当前为layer3</span></span><br><span class="line"><span class="comment"># 意味每个单元的保留概率是0.8 </span></span><br><span class="line">keep-prop = <span class="number">0.8</span> </span><br><span class="line"><span class="comment"># 生成一个随机矩阵，该矩阵和a3具有相同的行列数，且矩阵中 任一元素 d3[i,j] = 0的概率是0.2，等于1的概率是0.8</span></span><br><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>],a3.shape[<span class="number">1</span>])&lt;keep-prop</span><br><span class="line"><span class="comment"># 重新计算a3</span></span><br><span class="line">a3 = np.multiply(a3,d3)</span><br><span class="line"><span class="comment"># 除以keep-prop的目的是为了不减少a3的期望</span></span><br><span class="line">a3 /= keep-prop</span><br></pre></td></tr></table></figure>
<p>关于为什么a3要除以<strong>keep-prop</strong></p>
<p>这被称为<strong>inverted dropout</strong>。当模型使用了<strong>dropout layer</strong>，训练的时候只有占比为<strong><em>P</em></strong> 的隐藏层单元参与训练，那么在预测的时候，如果所有的隐藏层单元都需要参与进来，则得到的结果相比训练时平均要大<strong><em>1/p</em></strong> ，为了避免这种情况，就需要测试的时候将输出结果乘以  <strong><em>p</em></strong> 使下一层的输入规模保持不变。而利用<strong>inverted dropout</strong>，我们可以在训练的时候直接将<strong>dropout</strong>后留下的权重扩大<strong><em>1/p</em></strong>  倍，这样就可以使结果的<strong>scale</strong>保持不变，而在预测的时候也不用做额外的操作了，更方便一些。</p>
<p>或者更直观的解释</p>
<p>10个人拉一个10吨车，第一次（训练时），只有5个人出力（有p=0.5的人被dropout了），那么这5个人每个人出力拉2吨。第二次（预测时），10个人都被要求出力，这次每个人出的力就是2*（1-0.5）=1吨了 <a href="https://www.zhihu.com/question/61751133" target="_blank" rel="noopener">该解释转自知乎</a></p>
<p><strong>keep-prop</strong>的数值，对于不同的层，可以根据需要设置不同大小</p>
<p><img src="/media/15425968372625.jpg" alt=""></p>
<p>第二层的权重矩阵w[7×7]，占比最多，为了减少过拟合，可以将第二层<strong>的keep-prop</strong>设置的低一些，在4，5层 可以将<strong>keep_props</strong>设置的高一些。</p>
<p><strong>总结一下：</strong><br>如果你觉得 某一层相比其它层更容易出现过拟合，那么可以将该层的<strong>keep-prop</strong>设置的低一些，一般不会输入层施加<strong>drop-out</strong>，就算是施加，也尽量是<strong>keep-prop</strong>接近1。</p>
<p><strong>drop-out</strong>是对神经网络施加正则化的一种手段，它强迫模型不过度依赖于某一个神经元，使得权重在神经元之间平均分散，它可以使权重矩阵的<strong>F范数</strong>变小，但是会导致我们无法debug，因为施加<strong>drop-out</strong>之后，代价函数<strong><em>J</em></strong>便没有了一个确切的定义，我们无法得到一个单调递减的代价函数<strong><em>J</em></strong>，如果在某次迭代中出现了问题，也无法定位，因为每次的模型都是随机产生的。</p>
<p><strong>drop-out</strong>的作用是为了解决过拟合，所以如果模型不存在过拟合情况，可以不用，一般来说 在计算机视觉领域，<strong>drop-out</strong>会适用的比较频繁，因为CV的输入一般都是图片像素，因为输入太大了，导致数据量相比输入而言总显得过小，很容易出现过拟合的情况，所以<strong>drop-out</strong>在CV领域，基本是一个默认选项。</p>
<h2 id="dropout的优缺点"><a href="#dropout的优缺点" class="headerlink" title="dropout的优缺点"></a>dropout的优缺点</h2><h3 id="Dropout优点"><a href="#Dropout优点" class="headerlink" title="Dropout优点"></a>Dropout优点</h3><p><strong>计算方便</strong>。训练过程中使用<strong>Dropout</strong>产生 <strong>n</strong> 个随机二进制数与状态相乘即可。每个样本每次更新的时间复杂度： <strong><em>O(n)</em></strong>，空间复杂度： <strong><em>O(n)</em></strong>。<br><strong>适用广</strong>。<strong>Dropout</strong>不怎么限制适用的模型或训练过程，几乎在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很好。包括：前馈神经网络、概率模型、受限波尔兹曼机、循环神经网络等。<br>相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效。也可与其他形式的正则化合并，得到进一步提升。</p>
<h3 id="Dropout缺点"><a href="#Dropout缺点" class="headerlink" title="Dropout缺点"></a>Dropout缺点</h3><p><strong>不适合宽度太窄的网络</strong>。否则大部分网络没有输入到输出的路径。<br><strong>不适合训练数据太小（如小于5000）的网络</strong>。训练数据太小时，Dropout没有其他方法表现好。<br><strong>不适合非常大的数据集</strong>。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处</p>
<h2 id="其他的正则化方法"><a href="#其他的正则化方法" class="headerlink" title="其他的正则化方法"></a>其他的正则化方法</h2><h3 id="通过处理输入-比如对输入图片进行-翻转-剪裁-对数字进行扭曲-来人为增大数据集-减少误差"><a href="#通过处理输入-比如对输入图片进行-翻转-剪裁-对数字进行扭曲-来人为增大数据集-减少误差" class="headerlink" title="通过处理输入 比如对输入图片进行 翻转 剪裁 对数字进行扭曲 来人为增大数据集 减少误差"></a>通过处理输入 比如对输入图片进行 翻转 剪裁 对数字进行扭曲 来人为增大数据集 减少误差</h3><h3 id="early-stop"><a href="#early-stop" class="headerlink" title="early-stop"></a>early-stop</h3><p>在 testSet的误差开始变大的时候，及时停止</p>
<h2 id="正则化输入"><a href="#正则化输入" class="headerlink" title="正则化输入"></a>正则化输入</h2><p>假设输入的数据集，每一个输入有两个特征，则输入的散点图如下</p>
<p><img src="/media/15426027650538.jpg" alt=""></p>
<p>归一化输入 则需要两部</p>
<ol>
<li>零均值化： 计算输入的均值μ，则<br>X = X-μ<br>这样的目的是让输入值的均值为0<br><img src="/media/15426029182235.jpg" alt=""></li>
</ol>
<ol start="2">
<li>归一化方差<br>可以看到 零均值化后的输入 x1 和x2方向的方差差距还是挺大的</li>
</ol>
<p>通过计算所有数据的方差σ² = 1/m*Σ（x(i)^2）<br>再将所有向量除以σ,x1和 x2的方差就都变为1了</p>
<p><img src="/media/15426031977047.jpg" alt=""></p>
<p><img src="/media/15426032845833.jpg" alt=""></p>
<p>注意： 测试集和训练集请使用同样的归一化参数</p>
<h3 id="为什么要归一化输入"><a href="#为什么要归一化输入" class="headerlink" title="为什么要归一化输入"></a>为什么要归一化输入</h3><p>假设没有对参数做归一化处理</p>
<p>那么x1 和x2的输入范围可能差距非常大，假设x1取值[0,10000] 而x2 取值[0,1] 这样导致的结果是参数w1 和 w2 的范围差距同样非常大，这样会导致J[W,B] 的图像变得狭长，会导致梯度下降的速度变慢[注2]<br><img src="/media/15426035904351.jpg" alt=""></p>
<p>当对输入进行归一化处理后，图像更偏向与均匀的碗形。<br><img src="/media/15426036134992.jpg" alt=""></p>
<p>这样对代价函数J进行梯度下降时，会使得梯度下降的速度更快。</p>
<blockquote>
<p><strong>[注2]:</strong><br>梯度下降的原理是让<strong><em>w</em></strong>和<strong><em>b</em></strong> 沿着 <strong><em>j</em></strong>对<strong><em>w</em></strong>和<strong><em>b</em></strong> 的负梯度方向前进，知道接近全局最小值。<br>让图像形状狭长的时候，梯度的前进方向会更容易偏离最优值的方向，那么在梯度下降过程中，前进方向难免会有很多折线，这样会使的前进速度变慢。 当图像偏向更均匀的圆形时，前进方向更平缓，也更容易找到最优解。</p>
</blockquote>
<h2 id="梯度消失与梯度爆炸"><a href="#梯度消失与梯度爆炸" class="headerlink" title="梯度消失与梯度爆炸"></a>梯度消失与梯度爆炸</h2><p>深层网络由许多非线性层堆叠而来，每一层非线性层都可以视为是一个非线性函数 <strong><em>f(x)···f(x)f(x)</em></strong>(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数<br><img src="/media/15426057015192.jpg" alt=""></p>
<p>我们最终的目的是希望这个多元函数可以很好的完成输入到输出之间的映射，假设不同的输入，输出的最优解是<strong><em>g(x) g(x)g(x)</em></strong> ，那么，优化深度网络就是为了寻找到合适的权值，满足<strong><em>Loss=L(g(x),F(x)) Loss = L(g(x),F(x))Loss=L(g(x),F(x))</em></strong>取得极小值点，比如最简单的损失函数<br><img src="/media/15426057167589.jpg" alt=""></p>
<p>​    发生梯度爆炸、梯度消失的原因有两个<br>​<em> 网络层级太深<br>​</em> 采用了不合适的激活函数</p>
<p>例如，对于下图所示的含有3个隐藏层的神经网络，梯度消失问题发生时，接近于输出层的<strong><em>hidden layer 3</em></strong>等的权值更新相对正常，但前面的<strong><em>hidden layer 1</em></strong>的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致<strong><em>hidden layer 1</em></strong>相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。</p>
<p><img src="/media/15426066208782.jpg" alt=""></p>
<p>以下图的反向传播为例（假设每一层只有一个神经元且对于每一层<img src="/media/15426066801029.jpg" alt=""></p>
<p>可以推导出<br><img src="/media/15426066951424.jpg" alt=""><br>而sigmod的导数<br><img src="/media/15426067183425.jpg" alt=""></p>
<p>最大值为1/4 ，而初始化网络权值的时候，<strong><em>w</em></strong>通常小于1 ，导致<img src="/media/15426068404619.jpg" alt=""><br> 因此对于上面的链式求导，层数越多，求导结果越小，因而导致梯度消失的情况出现。梯度爆炸的原因正好相反，<br><img src="/media/15426068576215.jpg" alt=""></p>
<p>其实梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应</p>
<h2 id="合理初始化神经网络的权重值-Xavier初始化"><a href="#合理初始化神经网络的权重值-Xavier初始化" class="headerlink" title="合理初始化神经网络的权重值(Xavier初始化)"></a>合理初始化神经网络的权重值(<strong><em>Xavier</em></strong>初始化)</h2><p>使用不同的激活函数，对于参数矩阵w的初始化方法也不相同<br><a href="https://zhuanlan.zhihu.com/p/27919794" target="_blank" rel="noopener">具体可以看这篇文章</a>，详细阐述了<strong><em>Xavier</em></strong>的初始化原理。</p>
<h2 id="梯度逼近"><a href="#梯度逼近" class="headerlink" title="梯度逼近"></a>梯度逼近</h2><p><img src="/media/15426119280417.jpg" alt=""></p>
<p>使用双边公差来计算梯度，可以有效减少误差</p>
<h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>将所有<strong><em>w</em></strong>和<strong><em>b</em></strong> 排列为一个超级大的向量<strong><em>θ</em></strong>，对<strong><em>θ</em></strong>的每一项<strong><em>θi</em></strong>，依次执行梯度逼近，计算出每一项<strong><em>dθiapprox</em></strong></p>
<p><img src="/media/15426128355674.jpg" alt=""></p>
<p>检查<br><img src="/media/15426128643069.jpg" alt=""></p>
<p>是否在合理范围内。<br>比如 如果<strong><em>ξ</em></strong>取值为10^-7, 梯度检验的结果和该标准相差几个数量级，就要考虑算法中是不是出现了bug。</p>
<p>具体到例子中，代价函数关于参数的梯度的定义:<br><img src="/media/15426144971309.jpg" alt=""><br>【这里<strong><em>wi</em></strong> 和 <strong><em>θi</em></strong>是 一一对应的】<br>我们想要确保 <strong><em>∂J/∂wi</em></strong> 的计算是正确的，只需要取 <strong><em>ε</em></strong> 为一个很小的数 (例如 10^−7)，然后计算 <strong><em>(J(wi+ε)−J(wi−ε))/2ε</em></strong> 是否约等于<strong><em> ∂J/∂wi</em></strong>，实际操作中判断两个参数向量的欧氏距离是否足够小。</p>
<p><strong>对于每个参数 wi，实施梯度检验的步骤一般是：</strong></p>
<p>1 . 使用前向传播计算代价函数 <strong><em>J(wi+ε)</em></strong><br>2 . 使用前向传播计算代价函数 <strong><em>J(wi−ε)</em></strong><br>3 . 计算梯度的近似值 (数值微分) <strong><em>gradapprox[i]=(J(wi+ε)−J(wi−ε))/2ε</em></strong></p>
<ol start="4">
<li>使用链式法则计算反向传播梯度，缓存到变量 <strong><em>grad</em></strong> 中<br>使用以下公式计算梯度 <strong><em>grad</em></strong> 和梯度的近似值 <strong><em>gradapprox</em></strong> 的欧氏距离：<br><strong><em>difference=∣∣grad−gradapprox∣∣2 / ∣∣grad∣∣2+∣∣gradapprox||2</em></strong></li>
</ol>
<p>实施梯度检验，有是哪个基本原则：</p>
<h3 id="不要再训练中使用梯度检验"><a href="#不要再训练中使用梯度检验" class="headerlink" title="不要再训练中使用梯度检验"></a>不要再训练中使用梯度检验</h3><p>梯度检验只适用于调试，不要在训练中打开它<br>因为对每一个θ做数值逼近是非常消耗时间的，只在调试bug的时候打开它，在训练的时候关闭。</p>
<h3 id="如果梯度检验失败，要检查每一项dθiapprox的值。"><a href="#如果梯度检验失败，要检查每一项dθiapprox的值。" class="headerlink" title="如果梯度检验失败，要检查每一项dθiapprox的值。"></a>如果梯度检验失败，要检查每一项dθiapprox的值。</h3><p>这样可以发现，是在求导哪一个参数的过程中出了问题</p>
<h3 id="如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项"><a href="#如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项" class="headerlink" title="如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项"></a>如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项</h3><h3 id="dropout不可以和梯度检验同时使用"><a href="#dropout不可以和梯度检验同时使用" class="headerlink" title="dropout不可以和梯度检验同时使用"></a>dropout不可以和梯度检验同时使用</h3><p>因为每次迭代过程中 Dropout 会使神经元结点随机失活，难以计算 Dropout 在梯度下降上的代价函数 J。</p>
<p>实施梯度检验的python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Set-up variables</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".</span></span><br><span class="line">        thetaplus =  np.copy(parameters_values)                                       <span class="comment"># Step 1</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon                                   <span class="comment"># Step 2</span></span><br><span class="line">        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  <span class="comment"># Step 3</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)                                       <span class="comment"># Step 1</span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon                                 <span class="comment"># Step 2        </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) <span class="comment"># Step 3</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute gradapprox[i]</span></span><br><span class="line">        gradapprox[i] = (J_plus[i] - J_minus[i]) / (<span class="number">2</span> * epsilon)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compare gradapprox to backward propagation gradients by computing difference.</span></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                                     <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                                              <span class="comment"># Step 3'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:</span><br><span class="line">        print(<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure>

    </article>
    <!-- license  -->
    
        <div class="license-wrapper">
            <p>Author：<a href="http://wenyiqingnian.xyz">QuincyJiang</a>
            <p>原文链接：<a href="http://wenyiqingnian.xyz/2018/11/15/深度学习课程笔记 第一周 深度学习的实践层面/">http://wenyiqingnian.xyz/2018/11/15/深度学习课程笔记 第一周 深度学习的实践层面/</a>
            <p>发表日期：<a href="http://wenyiqingnian.xyz/2018/11/15/深度学习课程笔记 第一周 深度学习的实践层面/">November 15th 2018, 11:52:50 am</a>
            <p>更新日期：<a href="http://wenyiqingnian.xyz/2018/11/15/深度学习课程笔记 第一周 深度学习的实践层面/">November 19th 2018, 7:04:51 pm</a>
            <p>版权声明：本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可</p>
        </div>
    
    <!-- paginator  -->
    <ul class="post-paginator">
        <li class="next">
            
                <div class="nextSlogan">Next Post</div>
                <a href= "/2018/11/19/深度学习课程笔记 第二周 优化算法/" title= "深度学习课程笔记 第二周 优化算法">
                    <div class="nextTitle">深度学习课程笔记 第二周 优化算法</div>
                </a>
            
        </li>
        <li class="previous">
            
                <div class="prevSlogan">Previous Post</div>
                <a href= "/2018/11/11/机器学习算法的演进/" title= "机器学习算法的演进">
                    <div class="prevTitle">机器学习算法的演进</div>
                </a>
            
        </li>
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

    <div id="lv-container" data-id="city" data-uid= MTAyMC8zNjQwMS8xMjkzNg==>
        <script type="text/javascript">
            (function (d, s) {
                var j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') { return; }
                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    </div>

<!-- City版安装代码已完成 -->
    
    
    <!-- partial('_partial/comment/changyan') -->
    <!--PC版-->


    
    

    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:1083873272@qq.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/QuincyJiang" class="iconfont-archer github" target="_blank" title=github></a>
            
        
    
        
            
                <span class="iconfont-archer wechat" title=wechat>
                  
                  <img class="profile-qr" src="/assets/wechat.jpeg" />
                </span>
            
        
    
        
    
        
    
        
            
                <a href="https://weibo.com/2425393311/" class="iconfont-archer weibo" target="_blank" title=weibo></a>
            
        
    
        
    
        
    
        
    
        
    
        
            
                <a href="http://aquencyua11.lofter.com/" class="iconfont-archer instagram" target="_blank" title=instagram></a>
            
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">Archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
    <div class="busuanzi-container">
    
     
    <span id="busuanzi_container_site_pv">PV: <span id="busuanzi_value_site_pv"></span> :)</span>
    
    </div>
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper" style=
    







top:50vh;

    >
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#改善深层神经网络：-超参调试、正则化以及优化"><span class="toc-number">1.</span> <span class="toc-text">改善深层神经网络： 超参调试、正则化以及优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#训练集-开发集-测试集-的划分"><span class="toc-number">1.1.</span> <span class="toc-text">训练集 开发集 测试集 的划分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#保证-验证集合测试集具有相同的分布"><span class="toc-number">1.2.</span> <span class="toc-text">保证 验证集合测试集具有相同的分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么要保证分布相同"><span class="toc-number">1.3.</span> <span class="toc-text">为什么要保证分布相同</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何评估模型表现？-偏差、方差"><span class="toc-number">1.4.</span> <span class="toc-text">如何评估模型表现？ 偏差、方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一个基本流程"><span class="toc-number">1.5.</span> <span class="toc-text">一个基本流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#降低方差的手段：正则化"><span class="toc-number">1.6.</span> <span class="toc-text">降低方差的手段：正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#如何实施正则化"><span class="toc-number">1.6.1.</span> <span class="toc-text">如何实施正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#logistic-回归"><span class="toc-number">1.6.1.1.</span> <span class="toc-text">logistic 回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络"><span class="toc-number">1.6.1.2.</span> <span class="toc-text">神经网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么正则化可以减少过拟合"><span class="toc-number">1.6.2.</span> <span class="toc-text">为什么正则化可以减少过拟合</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#另一种正则化方式：-Dropout正则化"><span class="toc-number">1.7.</span> <span class="toc-text">另一种正则化方式： Dropout正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dropout的优缺点"><span class="toc-number">1.8.</span> <span class="toc-text">dropout的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout优点"><span class="toc-number">1.8.1.</span> <span class="toc-text">Dropout优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout缺点"><span class="toc-number">1.8.2.</span> <span class="toc-text">Dropout缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#其他的正则化方法"><span class="toc-number">1.9.</span> <span class="toc-text">其他的正则化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#通过处理输入-比如对输入图片进行-翻转-剪裁-对数字进行扭曲-来人为增大数据集-减少误差"><span class="toc-number">1.9.1.</span> <span class="toc-text">通过处理输入 比如对输入图片进行 翻转 剪裁 对数字进行扭曲 来人为增大数据集 减少误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#early-stop"><span class="toc-number">1.9.2.</span> <span class="toc-text">early-stop</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#正则化输入"><span class="toc-number">1.10.</span> <span class="toc-text">正则化输入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么要归一化输入"><span class="toc-number">1.10.1.</span> <span class="toc-text">为什么要归一化输入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度消失与梯度爆炸"><span class="toc-number">1.11.</span> <span class="toc-text">梯度消失与梯度爆炸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#合理初始化神经网络的权重值-Xavier初始化"><span class="toc-number">1.12.</span> <span class="toc-text">合理初始化神经网络的权重值(Xavier初始化)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度逼近"><span class="toc-number">1.13.</span> <span class="toc-text">梯度逼近</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度检验"><span class="toc-number">1.14.</span> <span class="toc-text">梯度检验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#不要再训练中使用梯度检验"><span class="toc-number">1.14.1.</span> <span class="toc-text">不要再训练中使用梯度检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如果梯度检验失败，要检查每一项dθiapprox的值。"><span class="toc-number">1.14.2.</span> <span class="toc-text">如果梯度检验失败，要检查每一项dθiapprox的值。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项"><span class="toc-number">1.14.3.</span> <span class="toc-text">如果代价函数有正则化项，求解dθ的时候不要漏掉这个正则化项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dropout不可以和梯度检验同时使用"><span class="toc-number">1.14.4.</span> <span class="toc-text">dropout不可以和梯度检验同时使用</span></a></li></ol></li></ol></li></ol>
    </div>
    
    <div class="back-top iconfont-archer">&#xe639;</div>
    <div class="sidebar sidebar-hide">
    <ul class="sidebar-tabs sidebar-tabs-active-0">
        <li class="sidebar-tab-archives"><span class="iconfont-archer">&#xe67d;</span><span class="tab-name">Archive</span></li>
        <li class="sidebar-tab-tags"><span class="iconfont-archer">&#xe61b;</span><span class="tab-name">Tag</span></li>
        <li class="sidebar-tab-categories"><span class="iconfont-archer">&#xe666;</span><span class="tab-name">Cate</span></li>
    </ul>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-panel-archives">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-and-search">
        <div class="total-archive">
        Total : 35
        </div>
        <!-- search  -->
        
    </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/21</span><a class="archive-post-title" href= "/2019/10/21/为何fork两次可以避免僵尸进程/" >为何fork两次可以避免僵尸进程</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/14</span><a class="archive-post-title" href= "/2019/01/14/mac 搭建Go开发环境/" >mac 搭建Go开发环境</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/19</span><a class="archive-post-title" href= "/2018/11/19/深度学习课程笔记 第二周 优化算法/" >深度学习课程笔记 第二周 优化算法</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/15</span><a class="archive-post-title" href= "/2018/11/15/深度学习课程笔记 第一周 深度学习的实践层面/" >深度学习课程笔记 第一周 深度学习的实践层面</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">11/11</span><a class="archive-post-title" href= "/2018/11/11/机器学习算法的演进/" >机器学习算法的演进</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/21</span><a class="archive-post-title" href= "/2018/08/21/八月份学习计划/" >下个阶段的学习计划整理</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/16</span><a class="archive-post-title" href= "/2018/08/16/一些知识点总结/" >一些知识点总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/2018/06/24/从一次native端的IPC流程理解binder/" >从一次native端的IPC流程理解binder</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/16</span><a class="archive-post-title" href= "/2018/06/16/《现代艺术150年》未影印作品2/" >《现代艺术150年》未影印作品2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/08</span><a class="archive-post-title" href= "/2018/06/08/深度学习1-反向传播/" >深度学习1-反向传播</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2018/06/02/《现代艺术150年》未影印作品1/" >《现代艺术150年》未影印作品1</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2018/06/02/从驱动角度理解binder/" >从驱动角度理解binder</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/02</span><a class="archive-post-title" href= "/2018/06/02/理解Liunx的FD与Inode/" >理解Linux的FD与Inode</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/30</span><a class="archive-post-title" href= "/2018/05/30/Docker入门/" >Docker入门</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/29</span><a class="archive-post-title" href= "/2018/05/29/Binder的设计架构/" >Binder的设计架构</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/23</span><a class="archive-post-title" href= "/2018/05/23/JAVA的垃圾回收策略（二）/" >JAVA的垃圾回收策略（二）</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/17</span><a class="archive-post-title" href= "/2018/05/17/JAVA的垃圾回收策略/" >JAVA的垃圾回收策略</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/14</span><a class="archive-post-title" href= "/2018/05/14/hexo+icarus/" >hexo+icarus</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/13</span><a class="archive-post-title" href= "/2018/05/13/六大设计模式/" >六大设计模式</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/10</span><a class="archive-post-title" href= "/2018/05/10/AnimatedVectorDrawable 总结/" >AnimatedVectorDrawable总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/12</span><a class="archive-post-title" href= "/2018/04/12/Rxjava2操作符/" >Rxjava2操作符</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/16</span><a class="archive-post-title" href= "/2018/03/16/Socket未释放导致的句柄泄露/" >socket未释放导致句柄泄露</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/13</span><a class="archive-post-title" href= "/2018/03/13/线程阻塞和中断的四种方式/" >线程阻塞和中断的四种方式</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/11</span><a class="archive-post-title" href= "/2017/12/11/MQTT相关/" >MQTT相关总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/11</span><a class="archive-post-title" href= "/2017/12/11/mac搭建PyQt5环境/" >mac搭建Pyqt5环境</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">10/12</span><a class="archive-post-title" href= "/2017/10/12/view 绘制机制/" >view 绘制机制</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/07</span><a class="archive-post-title" href= "/2017/09/07/Linux 用户空间 内核空间/" >linux用户控件、内和空间</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/16</span><a class="archive-post-title" href= "/2017/08/16/webrtc 音频/" >webrtc音频总结</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/18</span><a class="archive-post-title" href= "/2017/06/18/linux的地址映射/" >linux的地址映射</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/07</span><a class="archive-post-title" href= "/2017/05/07/MK语法规范/" >MK语法规范</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/11</span><a class="archive-post-title" href= "/2017/04/11/Binder 进程间通讯机制/" >Binder通讯机制</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/05</span><a class="archive-post-title" href= "/2017/04/05/编译系统环境初始化过程/" >编译系统环境初始化过程</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/15</span><a class="archive-post-title" href= "/2017/03/15/添加SE安全策略/" >添加SE安全策略</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2016 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/03</span><a class="archive-post-title" href= "/2016/08/03/抓包工具 - Fiddler（如何捕获Android数据包）/" >fiddler抓android数据包</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/04</span><a class="archive-post-title" href= "/2016/07/04/Linux vi命令使用方法/" >Linux vi用法</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-panel-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name" data-tags="句柄animation"><span class="iconfont-archer">&#xe606;</span>句柄animation</span>
    
        <span class="sidebar-tag-name" data-tags="AnimatedVectorDrawable"><span class="iconfont-archer">&#xe606;</span>AnimatedVectorDrawable</span>
    
        <span class="sidebar-tag-name" data-tags="android"><span class="iconfont-archer">&#xe606;</span>android</span>
    
        <span class="sidebar-tag-name" data-tags="Binder"><span class="iconfont-archer">&#xe606;</span>Binder</span>
    
        <span class="sidebar-tag-name" data-tags="java"><span class="iconfont-archer">&#xe606;</span>java</span>
    
        <span class="sidebar-tag-name" data-tags="垃圾回收"><span class="iconfont-archer">&#xe606;</span>垃圾回收</span>
    
        <span class="sidebar-tag-name" data-tags="docker"><span class="iconfont-archer">&#xe606;</span>docker</span>
    
        <span class="sidebar-tag-name" data-tags="容器"><span class="iconfont-archer">&#xe606;</span>容器</span>
    
        <span class="sidebar-tag-name" data-tags="linux命令"><span class="iconfont-archer">&#xe606;</span>linux命令</span>
    
        <span class="sidebar-tag-name" data-tags="mqtt"><span class="iconfont-archer">&#xe606;</span>mqtt</span>
    
        <span class="sidebar-tag-name" data-tags="编译"><span class="iconfont-archer">&#xe606;</span>编译</span>
    
        <span class="sidebar-tag-name" data-tags="mk"><span class="iconfont-archer">&#xe606;</span>mk</span>
    
        <span class="sidebar-tag-name" data-tags="hexo"><span class="iconfont-archer">&#xe606;</span>hexo</span>
    
        <span class="sidebar-tag-name" data-tags="开源框架"><span class="iconfont-archer">&#xe606;</span>开源框架</span>
    
        <span class="sidebar-tag-name" data-tags="rxjava2"><span class="iconfont-archer">&#xe606;</span>rxjava2</span>
    
        <span class="sidebar-tag-name" data-tags="linux"><span class="iconfont-archer">&#xe606;</span>linux</span>
    
        <span class="sidebar-tag-name" data-tags="句柄泄露"><span class="iconfont-archer">&#xe606;</span>句柄泄露</span>
    
        <span class="sidebar-tag-name" data-tags="bugs"><span class="iconfont-archer">&#xe606;</span>bugs</span>
    
        <span class="sidebar-tag-name" data-tags="webrtc"><span class="iconfont-archer">&#xe606;</span>webrtc</span>
    
        <span class="sidebar-tag-name" data-tags="后端技术"><span class="iconfont-archer">&#xe606;</span>后端技术</span>
    
        <span class="sidebar-tag-name" data-tags="go"><span class="iconfont-archer">&#xe606;</span>go</span>
    
        <span class="sidebar-tag-name" data-tags="现代艺术150年"><span class="iconfont-archer">&#xe606;</span>现代艺术150年</span>
    
        <span class="sidebar-tag-name" data-tags="备忘录"><span class="iconfont-archer">&#xe606;</span>备忘录</span>
    
        <span class="sidebar-tag-name" data-tags="python"><span class="iconfont-archer">&#xe606;</span>python</span>
    
        <span class="sidebar-tag-name" data-tags="qt"><span class="iconfont-archer">&#xe606;</span>qt</span>
    
        <span class="sidebar-tag-name" data-tags="音视频"><span class="iconfont-archer">&#xe606;</span>音视频</span>
    
        <span class="sidebar-tag-name" data-tags="学习计划"><span class="iconfont-archer">&#xe606;</span>学习计划</span>
    
        <span class="sidebar-tag-name" data-tags="设计模式"><span class="iconfont-archer">&#xe606;</span>设计模式</span>
    
        <span class="sidebar-tag-name" data-tags="机器学习"><span class="iconfont-archer">&#xe606;</span>机器学习</span>
    
        <span class="sidebar-tag-name" data-tags="Tensorflow"><span class="iconfont-archer">&#xe606;</span>Tensorflow</span>
    
        <span class="sidebar-tag-name" data-tags="抓包"><span class="iconfont-archer">&#xe606;</span>抓包</span>
    
        <span class="sidebar-tag-name" data-tags="SEAndroid"><span class="iconfont-archer">&#xe606;</span>SEAndroid</span>
    
        <span class="sidebar-tag-name" data-tags="多线程"><span class="iconfont-archer">&#xe606;</span>多线程</span>
    
        <span class="sidebar-tag-name" data-tags="aosp"><span class="iconfont-archer">&#xe606;</span>aosp</span>
    
    </div>
    <div class="iconfont-archer sidebar-tags-empty">&#xe678;</div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: true
    tags: true</pre>
    </div> 
    <div class="sidebar-tags-list"></div>
</div>
        <div class="sidebar-panel-categories">
    <div class="sidebar-categories-name">
    
        <span class="sidebar-category-name" data-categories="CoolUI"><span class="iconfont-archer">&#xe60a;</span>CoolUI</span>
    
        <span class="sidebar-category-name" data-categories="Android"><span class="iconfont-archer">&#xe60a;</span>Android</span>
    
        <span class="sidebar-category-name" data-categories="java基础"><span class="iconfont-archer">&#xe60a;</span>java基础</span>
    
        <span class="sidebar-category-name" data-categories="容器技术"><span class="iconfont-archer">&#xe60a;</span>容器技术</span>
    
        <span class="sidebar-category-name" data-categories="Linux"><span class="iconfont-archer">&#xe60a;</span>Linux</span>
    
        <span class="sidebar-category-name" data-categories="MQTT"><span class="iconfont-archer">&#xe60a;</span>MQTT</span>
    
        <span class="sidebar-category-name" data-categories="AOSP"><span class="iconfont-archer">&#xe60a;</span>AOSP</span>
    
        <span class="sidebar-category-name" data-categories="备忘录"><span class="iconfont-archer">&#xe60a;</span>备忘录</span>
    
        <span class="sidebar-category-name" data-categories="bug记录"><span class="iconfont-archer">&#xe60a;</span>bug记录</span>
    
        <span class="sidebar-category-name" data-categories="webrtc"><span class="iconfont-archer">&#xe60a;</span>webrtc</span>
    
        <span class="sidebar-category-name" data-categories="后端技术"><span class="iconfont-archer">&#xe60a;</span>后端技术</span>
    
        <span class="sidebar-category-name" data-categories="python"><span class="iconfont-archer">&#xe60a;</span>python</span>
    
        <span class="sidebar-category-name" data-categories="学习计划"><span class="iconfont-archer">&#xe60a;</span>学习计划</span>
    
        <span class="sidebar-category-name" data-categories="机器学习"><span class="iconfont-archer">&#xe60a;</span>机器学习</span>
    
    </div>
    <div class="iconfont-archer sidebar-categories-empty">&#xe678;</div>
    <div class="sidebar-categories-list"></div>
</div>
    </div>
</div> 
    <script>
    var siteMeta = {
        root: "/",
        author: "QuincyJiang"
    }
</script>
    <!-- CDN failover -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
    <script type="text/javascript">
        if (typeof window.$ === 'undefined')
        {
            console.warn('jquery load from jsdelivr failed, will load local script')
            document.write('<script src="/lib/jquery.min.js">\x3C/script>')
        }
    </script>
    <script src="/scripts/main.js"></script>
    <!-- algolia -->
    
    <!-- busuanzi  -->
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
    <!-- CNZZ  -->
    
    </div>
    <!-- async load share.js -->
    
        <script src="/scripts/share.js" async></script>    
     
    </body>
</html>


